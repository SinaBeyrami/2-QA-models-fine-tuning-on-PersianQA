{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiC5BR4n7iV5"
   },
   "source": [
    "**Installing requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:38:52.457418Z",
     "iopub.status.busy": "2025-07-20T04:38:52.457144Z",
     "iopub.status.idle": "2025-07-20T04:38:55.788807Z",
     "shell.execute_reply": "2025-07-20T04:38:55.787717Z",
     "shell.execute_reply.started": "2025-07-20T04:38:52.457399Z"
    },
    "id": "m24C1gd47iV6",
    "outputId": "c5c17e2a-8b44-4837-8506-7590499b42d6",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRpIQf3f7iV6"
   },
   "source": [
    "**Turn off Wandb (For reporting and needs API key)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:38:55.790904Z",
     "iopub.status.busy": "2025-07-20T04:38:55.790661Z",
     "iopub.status.idle": "2025-07-20T04:38:55.795638Z",
     "shell.execute_reply": "2025-07-20T04:38:55.795065Z",
     "shell.execute_reply.started": "2025-07-20T04:38:55.790883Z"
    },
    "id": "_c7LJTRF7iV6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQP4FATW7iV7"
   },
   "source": [
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-20T04:38:55.796599Z",
     "iopub.status.busy": "2025-07-20T04:38:55.796369Z",
     "iopub.status.idle": "2025-07-20T04:38:55.815959Z",
     "shell.execute_reply": "2025-07-20T04:38:55.815316Z",
     "shell.execute_reply.started": "2025-07-20T04:38:55.796583Z"
    },
    "id": "CbBdALzV7iV7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json, numpy as np, torch, csv, pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    AutoTokenizer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CE4nwqk7iV7"
   },
   "source": [
    "**Model loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:38:55.818150Z",
     "iopub.status.busy": "2025-07-20T04:38:55.817923Z",
     "iopub.status.idle": "2025-07-20T04:38:56.316583Z",
     "shell.execute_reply": "2025-07-20T04:38:56.315977Z",
     "shell.execute_reply.started": "2025-07-20T04:38:55.818126Z"
    },
    "id": "0EBg6DXC7iV7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"pedramyazdipoor/parsbert_question_answering_PQuAD\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:38:56.317552Z",
     "iopub.status.busy": "2025-07-20T04:38:56.317270Z",
     "iopub.status.idle": "2025-07-20T04:38:56.914369Z",
     "shell.execute_reply": "2025-07-20T04:38:56.913613Z",
     "shell.execute_reply.started": "2025-07-20T04:38:56.317527Z"
    },
    "id": "5O4gvJWi7iV7",
    "outputId": "e6d351fb-6bb7-463a-837f-8b385bae81ef",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pedramyazdipoor/parsbert_question_answering_PQuAD were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at pedramyazdipoor/parsbert_question_answering_PQuAD and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdR9rl_77iV7"
   },
   "source": [
    "**Data loading and dataset making**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:38:56.920826Z",
     "iopub.status.busy": "2025-07-20T04:38:56.920239Z",
     "iopub.status.idle": "2025-07-20T04:38:56.938242Z",
     "shell.execute_reply": "2025-07-20T04:38:56.937521Z",
     "shell.execute_reply.started": "2025-07-20T04:38:56.920796Z"
    },
    "id": "sfCBe1qx7iV7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_pqa(path):\n",
    "    raw = load_dataset(\"json\", data_files=path, field=\"data\")[\"train\"]\n",
    "    flat_rows = []\n",
    "    for art in raw:\n",
    "        for para in art[\"paragraphs\"]:\n",
    "            ctx = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                flat_rows.append({\n",
    "                    \"id\":           str(qa[\"id\"]),\n",
    "                    \"question\":     qa[\"question\"],\n",
    "                    \"context\":      ctx,\n",
    "                    \"is_impossible\": qa[\"is_impossible\"],\n",
    "                    \"answer_text\":   qa[\"answers\"][0][\"text\"]        if not qa[\"is_impossible\"] else \"\",\n",
    "                    \"answer_start\":  qa[\"answers\"][0][\"answer_start\"] if not qa[\"is_impossible\"] else 0,\n",
    "                })\n",
    "\n",
    "    return Dataset.from_list(flat_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:38:56.939161Z",
     "iopub.status.busy": "2025-07-20T04:38:56.938875Z",
     "iopub.status.idle": "2025-07-20T04:38:57.769950Z",
     "shell.execute_reply": "2025-07-20T04:38:57.769227Z",
     "shell.execute_reply.started": "2025-07-20T04:38:56.939127Z"
    },
    "id": "E_F9YvEM7iV7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = load_pqa(\"/kaggle/input/pqa-dataset/pqa_train.json\")\n",
    "test_ds  = load_pqa(\"/kaggle/input/pqa-dataset/pqa_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:38:57.770955Z",
     "iopub.status.busy": "2025-07-20T04:38:57.770759Z",
     "iopub.status.idle": "2025-07-20T04:38:57.775500Z",
     "shell.execute_reply": "2025-07-20T04:38:57.774940Z",
     "shell.execute_reply.started": "2025-07-20T04:38:57.770939Z"
    },
    "id": "5UxwL4yV7iV8",
    "outputId": "95fc1416-3cc8-4f50-e9d9-cfbd0a00ea09",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'question', 'context', 'is_impossible', 'answer_text', 'answer_start'],\n",
      "    num_rows: 9008\n",
      "})\n",
      "{'id': '1', 'question': 'شرکت فولاد مبارکه در کجا واقع شده است', 'context': 'شرکت فولاد مبارکۀ اصفهان، بزرگ\\u200cترین واحد صنعتی خصوصی در ایران و بزرگ\\u200cترین مجتمع تولید فولاد در کشور ایران است، که در شرق شهر مبارکه قرار دارد. فولاد مبارکه هم\\u200cاکنون محرک بسیاری از صنایع بالادستی و پایین\\u200cدستی است. فولاد مبارکه در ۱۱ دوره جایزۀ ملی تعالی سازمانی و ۶ دوره جایزۀ شرکت دانشی در کشور رتبۀ نخست را بدست آورده\\u200cاست و همچنین این شرکت در سال ۱۳۹۱ برای نخستین\\u200cبار به عنوان تنها شرکت ایرانی با کسب امتیاز ۶۵۴ تندیس زرین جایزۀ ملی تعالی سازمانی را از آن خود کند. شرکت فولاد مبارکۀ اصفهان در ۲۳ دی ماه ۱۳۷۱ احداث شد و اکنون بزرگ\\u200cترین واحدهای صنعتی و بزرگترین مجتمع تولید فولاد در ایران است. این شرکت در زمینی به مساحت ۳۵ کیلومتر مربع در نزدیکی شهر مبارکه و در ۷۵ کیلومتری جنوب غربی شهر اصفهان واقع شده\\u200cاست. مصرف آب این کارخانه در کمترین میزان خود، ۱٫۵٪ از دبی زاینده\\u200cرود برابر سالانه ۲۳ میلیون متر مکعب در سال است و خود یکی از عوامل کم\\u200cآبی زاینده\\u200cرود شناخته می\\u200cشود.', 'is_impossible': False, 'answer_text': 'در شرق شهر مبارکه', 'answer_start': 114}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds)\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqJ6zLOu9N4x"
   },
   "source": [
    "**Tokenization and Converting dataset to model features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh5d-Fyt74bo"
   },
   "outputs": [],
   "source": [
    "max_len = 384\n",
    "stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:49:02.069165Z",
     "iopub.status.busy": "2025-07-20T04:49:02.068826Z",
     "iopub.status.idle": "2025-07-20T04:49:02.077899Z",
     "shell.execute_reply": "2025-07-20T04:49:02.077244Z",
     "shell.execute_reply.started": "2025-07-20T04:49:02.069142Z"
    },
    "id": "RM5cJg5l7iV8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    enc = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_len,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map   = enc.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_map   = enc[\"offset_mapping\"]\n",
    "\n",
    "    ids, starts, ends, ctxs, ex_ids = [], [], [], [], []\n",
    "\n",
    "    for i, offsets in enumerate(offset_map):\n",
    "        orig_idx   = sample_map[i]\n",
    "        ex_ids.append(examples[\"id\"][orig_idx])\n",
    "        ctxs.append(examples[\"context\"][orig_idx])\n",
    "\n",
    "        token_start = token_end = 0\n",
    "\n",
    "        if not examples[\"is_impossible\"][orig_idx]:\n",
    "            seq_ids   = enc.sequence_ids(i)\n",
    "            ctx_start = seq_ids.index(1)\n",
    "            ctx_end   = len(seq_ids) - 1 - seq_ids[::-1].index(1)\n",
    "\n",
    "            ans_char_start = examples[\"answer_start\"][orig_idx]\n",
    "            ans_char_end   = ans_char_start + len(examples[\"answer_text\"][orig_idx])\n",
    "\n",
    "            if offsets[ctx_start][0] <= ans_char_start < offsets[ctx_end][1]:\n",
    "                for idx in range(ctx_start, ctx_end + 1):\n",
    "                    if offsets[idx][0] <= ans_char_start < offsets[idx][1]:\n",
    "                        token_start = idx\n",
    "                    if offsets[idx][0] < ans_char_end <= offsets[idx][1]:\n",
    "                        token_end = idx\n",
    "                        break\n",
    "\n",
    "        ids.append(examples[\"id\"][orig_idx])\n",
    "        starts.append(token_start)\n",
    "        ends.append(token_end)\n",
    "\n",
    "    enc[\"id\"] = ids\n",
    "    enc[\"start_positions\"] = starts\n",
    "    enc[\"end_positions\"] = ends\n",
    "    enc[\"context\"] = ctxs\n",
    "    enc[\"example_id\"] = ex_ids\n",
    "\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "686a491ac4f346eb971460681c3c27ba",
      "d2b96338ff4a480d8bebdb7d7eedb924"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-07-20T04:49:06.330722Z",
     "iopub.status.busy": "2025-07-20T04:49:06.329946Z",
     "iopub.status.idle": "2025-07-20T04:49:14.922484Z",
     "shell.execute_reply": "2025-07-20T04:49:14.921741Z",
     "shell.execute_reply.started": "2025-07-20T04:49:06.330695Z"
    },
    "id": "rWoVC1DM7iV8",
    "outputId": "05b46993-50dc-4de3-d243-9b30435d0a1a",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686a491ac4f346eb971460681c3c27ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b96338ff4a480d8bebdb7d7eedb924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_feat = train_ds.map(\n",
    "    encode,\n",
    "    batched=True,\n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "test_feat = test_ds.map(\n",
    "    encode,\n",
    "    batched=True,\n",
    "    remove_columns=test_ds.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-20T04:38:58.773754Z",
     "iopub.status.idle": "2025-07-20T04:38:58.774102Z",
     "shell.execute_reply": "2025-07-20T04:38:58.773911Z",
     "shell.execute_reply.started": "2025-07-20T04:38:58.773896Z"
    },
    "id": "YbMPwLAx7iV8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_feat[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRLqC5cc9q_n"
   },
   "source": [
    "**Convert model logits to answer text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-20T04:38:58.775797Z",
     "iopub.status.idle": "2025-07-20T04:38:58.776209Z",
     "shell.execute_reply": "2025-07-20T04:38:58.776017Z",
     "shell.execute_reply.started": "2025-07-20T04:38:58.775999Z"
    },
    "id": "Ltu2ecay7iV8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def postprocess(predictions, features):\n",
    "    start_logits, end_logits = predictions\n",
    "    answers = []\n",
    "    for i, (s_log, e_log) in enumerate(zip(start_logits, end_logits)):\n",
    "        s = int(np.argmax(s_log)); e = int(np.argmax(e_log))\n",
    "        if s == 0 and e == 0:\n",
    "            answers.append({\"id\": str(features[\"id\"][i]), \"prediction_text\": \"\"})\n",
    "        else:\n",
    "            text = tokenizer.decode(\n",
    "                features[\"input_ids\"][i][s:e+1],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "            answers.append({\"id\": str(features[\"id\"][i]), \"prediction_text\": text.strip()})\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBWRlsbh95Lo"
   },
   "source": [
    "**Compact EM & F1 computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-20T04:38:58.777122Z",
     "iopub.status.idle": "2025-07-20T04:38:58.777441Z",
     "shell.execute_reply": "2025-07-20T04:38:58.777294Z",
     "shell.execute_reply.started": "2025-07-20T04:38:58.777279Z"
    },
    "id": "fzWpWY3L7iV8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fast_metric = evaluate.load(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:53:18.698346Z",
     "iopub.status.busy": "2025-07-20T04:53:18.698055Z",
     "iopub.status.idle": "2025-07-20T04:53:18.707568Z",
     "shell.execute_reply": "2025-07-20T04:53:18.706792Z",
     "shell.execute_reply.started": "2025-07-20T04:53:18.698326Z"
    },
    "id": "Od3cLX927iV8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_fast_metrics_fn(examples, features):\n",
    "    ctxs        = features[\"context\"]\n",
    "    offsets     = features[\"offset_mapping\"]\n",
    "    ex_ids      = features[\"example_id\"]\n",
    "    cls_indices = [ids.index(tokenizer.cls_token_id) for ids in features[\"input_ids\"]]\n",
    "\n",
    "    def compute_fast(pred_pack):\n",
    "        start_logits, end_logits = pred_pack.predictions\n",
    "        best = {}\n",
    "\n",
    "        for i in range(len(start_logits)):\n",
    "            s = int(np.argmax(start_logits[i]))\n",
    "            e = int(np.argmax(end_logits[i]))\n",
    "            cls = cls_indices[i]\n",
    "\n",
    "            if s == cls or e == cls or s > e:\n",
    "                score = start_logits[i][cls] + end_logits[i][cls]\n",
    "                text  = \"\"\n",
    "            else:\n",
    "                score = start_logits[i][s] + end_logits[i][e]\n",
    "                start_char = offsets[i][s][0]\n",
    "                end_char   = offsets[i][e][1]\n",
    "                text = ctxs[i][start_char:end_char]\n",
    "\n",
    "            eid = ex_ids[i]\n",
    "            if (eid not in best) or (score > best[eid][0]):\n",
    "                best[eid] = (score, text)\n",
    "\n",
    "        preds = [\n",
    "            {\"id\": k, \"prediction_text\": v[1], \"no_answer_probability\": 0.0}\n",
    "            for k, v in best.items()\n",
    "        ]\n",
    "        refs = [\n",
    "            {\"id\": ex[\"id\"],\n",
    "             \"answers\": {\n",
    "                 \"text\": [ex[\"answer_text\"]] if ex[\"answer_text\"] else [],\n",
    "                 \"answer_start\": [ex[\"answer_start\"]] if ex[\"answer_text\"] else [],\n",
    "             },\n",
    "            }\n",
    "            for ex in examples\n",
    "        ]\n",
    "        return fast_metric.compute(predictions=preds, references=refs)\n",
    "\n",
    "    return compute_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT1cR4x_-Hxf"
   },
   "source": [
    "**Log train/val loss to CSV during training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-20T04:38:58.779611Z",
     "iopub.status.idle": "2025-07-20T04:38:58.779825Z",
     "shell.execute_reply": "2025-07-20T04:38:58.779729Z",
     "shell.execute_reply.started": "2025-07-20T04:38:58.779721Z"
    },
    "id": "uweZtK7b7iV8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LossTrackerCallback(TrainerCallback):\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)\n",
    "\n",
    "        with open(self.csv_path, \"w\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([\"step\", \"train_loss\", \"eval_loss\"])\n",
    "\n",
    "        self.history = {\"step\": [], \"train_loss\": [], \"eval_loss\": []}\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kw):\n",
    "        if logs is None:\n",
    "            return\n",
    "        step = state.global_step\n",
    "        train_loss = logs.get(\"loss\")\n",
    "        eval_loss  = logs.get(\"eval_loss\")\n",
    "        if train_loss is None and eval_loss is None:\n",
    "            return\n",
    "\n",
    "        self.history[\"step\"].append(step)\n",
    "        self.history[\"train_loss\"].append(train_loss)\n",
    "        self.history[\"eval_loss\"].append(eval_loss)\n",
    "\n",
    "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([step, train_loss, eval_loss])\n",
    "\n",
    "    def on_train_end(self, *a, **kw):\n",
    "        df = pd.DataFrame(self.history)\n",
    "        print(\"\\nSample:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4Wg9ax_-PPP"
   },
   "source": [
    "**GPU check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-20T04:38:58.780430Z",
     "iopub.status.idle": "2025-07-20T04:38:58.780668Z",
     "shell.execute_reply": "2025-07-20T04:38:58.780572Z",
     "shell.execute_reply.started": "2025-07-20T04:38:58.780563Z"
    },
    "id": "WwccGlCb7iV8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUvPHe6z-bcH"
   },
   "source": [
    "**5-Fold Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "a6452d51c8c9454a9f7311120fa1dc7a",
      "fb642cd9c3934efb89ec2bf0c63fee9c"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-07-20T04:55:12.016763Z",
     "iopub.status.busy": "2025-07-20T04:55:12.015954Z",
     "iopub.status.idle": "2025-07-20T05:27:52.411318Z",
     "shell.execute_reply": "2025-07-20T05:27:52.410287Z",
     "shell.execute_reply.started": "2025-07-20T04:55:12.016741Z"
    },
    "id": "QKER5CUn7iV8",
    "outputId": "6037275a-fc23-4d8d-d69d-dfccd7f3782d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5 — 7230 train • 1808 val\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6452d51c8c9454a9f7311120fa1dc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pedramyazdipoor/parsbert_question_answering_PQuAD were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at pedramyazdipoor/parsbert_question_answering_PQuAD and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_36/2309679723.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4520' max='4520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4520/4520 28:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.414300</td>\n",
       "      <td>2.318261</td>\n",
       "      <td>32.190265</td>\n",
       "      <td>47.343360</td>\n",
       "      <td>1808</td>\n",
       "      <td>19.415943</td>\n",
       "      <td>41.039301</td>\n",
       "      <td>1267</td>\n",
       "      <td>62.107209</td>\n",
       "      <td>62.107209</td>\n",
       "      <td>541</td>\n",
       "      <td>32.245575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.398670</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.540800</td>\n",
       "      <td>2.167220</td>\n",
       "      <td>38.938053</td>\n",
       "      <td>53.859580</td>\n",
       "      <td>1808</td>\n",
       "      <td>23.756906</td>\n",
       "      <td>45.049819</td>\n",
       "      <td>1267</td>\n",
       "      <td>74.491682</td>\n",
       "      <td>74.491682</td>\n",
       "      <td>541</td>\n",
       "      <td>38.938053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.859580</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.403900</td>\n",
       "      <td>2.110079</td>\n",
       "      <td>39.823009</td>\n",
       "      <td>54.523607</td>\n",
       "      <td>1808</td>\n",
       "      <td>22.178374</td>\n",
       "      <td>43.156023</td>\n",
       "      <td>1267</td>\n",
       "      <td>81.146026</td>\n",
       "      <td>81.146026</td>\n",
       "      <td>541</td>\n",
       "      <td>39.823009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.523607</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.783800</td>\n",
       "      <td>2.508455</td>\n",
       "      <td>40.431416</td>\n",
       "      <td>55.461659</td>\n",
       "      <td>1808</td>\n",
       "      <td>24.704025</td>\n",
       "      <td>46.152075</td>\n",
       "      <td>1267</td>\n",
       "      <td>77.264325</td>\n",
       "      <td>77.264325</td>\n",
       "      <td>541</td>\n",
       "      <td>40.431416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.461659</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>2.472323</td>\n",
       "      <td>40.099558</td>\n",
       "      <td>55.960686</td>\n",
       "      <td>1808</td>\n",
       "      <td>24.151539</td>\n",
       "      <td>46.785257</td>\n",
       "      <td>1267</td>\n",
       "      <td>77.449168</td>\n",
       "      <td>77.449168</td>\n",
       "      <td>541</td>\n",
       "      <td>40.099558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.960686</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.400800</td>\n",
       "      <td>3.197503</td>\n",
       "      <td>38.440265</td>\n",
       "      <td>55.736728</td>\n",
       "      <td>1808</td>\n",
       "      <td>25.414365</td>\n",
       "      <td>50.096293</td>\n",
       "      <td>1267</td>\n",
       "      <td>68.946396</td>\n",
       "      <td>68.946396</td>\n",
       "      <td>541</td>\n",
       "      <td>38.440265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.736728</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>3.137213</td>\n",
       "      <td>39.988938</td>\n",
       "      <td>56.324661</td>\n",
       "      <td>1808</td>\n",
       "      <td>24.861878</td>\n",
       "      <td>48.172839</td>\n",
       "      <td>1267</td>\n",
       "      <td>75.415896</td>\n",
       "      <td>75.415896</td>\n",
       "      <td>541</td>\n",
       "      <td>39.988938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.324661</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>3.426843</td>\n",
       "      <td>39.988938</td>\n",
       "      <td>56.262986</td>\n",
       "      <td>1808</td>\n",
       "      <td>24.940805</td>\n",
       "      <td>48.163756</td>\n",
       "      <td>1267</td>\n",
       "      <td>75.231054</td>\n",
       "      <td>75.231054</td>\n",
       "      <td>541</td>\n",
       "      <td>39.988938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.262986</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>3.439315</td>\n",
       "      <td>40.044248</td>\n",
       "      <td>56.105845</td>\n",
       "      <td>1808</td>\n",
       "      <td>24.704025</td>\n",
       "      <td>47.623810</td>\n",
       "      <td>1267</td>\n",
       "      <td>75.970425</td>\n",
       "      <td>75.970425</td>\n",
       "      <td>541</td>\n",
       "      <td>40.044248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.105845</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample:\n",
      "    step  train_loss  eval_loss\n",
      "0     1      5.8430        NaN\n",
      "1   100      5.1491        NaN\n",
      "2   200      3.3799        NaN\n",
      "3   300      2.6790        NaN\n",
      "4   400      2.5737        NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [226/226 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 metrics: {'eval_loss': 2.110079288482666, 'eval_exact': 39.823008849557525, 'eval_f1': 54.52360665771219, 'eval_total': 1808, 'eval_HasAns_exact': 22.17837411207577, 'eval_HasAns_f1': 43.15602276017662, 'eval_HasAns_total': 1267, 'eval_NoAns_exact': 81.1460258780037, 'eval_NoAns_f1': 81.1460258780037, 'eval_NoAns_total': 541, 'eval_best_exact': 39.823008849557525, 'eval_best_exact_thresh': 0.0, 'eval_best_f1': 54.52360665771212, 'eval_best_f1_thresh': 0.0, 'eval_runtime': 23.3167, 'eval_samples_per_second': 77.541, 'eval_steps_per_second': 9.693, 'epoch': 5.0}\n",
      "Fold 2/5 — 7230 train • 1808 val\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb642cd9c3934efb89ec2bf0c63fee9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pedramyazdipoor/parsbert_question_answering_PQuAD were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at pedramyazdipoor/parsbert_question_answering_PQuAD and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_36/2309679723.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='4520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/4520 02:44 < 22:08, 3.03 it/s, Epoch 0.55/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.359800</td>\n",
       "      <td>2.159909</td>\n",
       "      <td>36.579967</td>\n",
       "      <td>49.034226</td>\n",
       "      <td>1807</td>\n",
       "      <td>20.527157</td>\n",
       "      <td>38.502274</td>\n",
       "      <td>1252</td>\n",
       "      <td>72.792793</td>\n",
       "      <td>72.792793</td>\n",
       "      <td>555</td>\n",
       "      <td>36.635307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.034226</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:626] . unexpected pos 623411392 vs 623411280",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/2: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/2309679723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mfold_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2620\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2623\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m                             \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3335\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3336\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3337\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3339\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 623411392 vs 623411280"
     ]
    }
   ],
   "source": [
    "kfold   = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for fold, (tr_idx, vl_idx) in enumerate(kfold.split(train_feat)):\n",
    "    print(f\"Fold {fold+1}/5 — {len(tr_idx)} train • {len(vl_idx)} val\")\n",
    "\n",
    "    vl_set = train_feat.select(vl_idx.tolist())\n",
    "\n",
    "    val_ids = set(vl_set[\"id\"])\n",
    "    raw_val = train_ds.filter(lambda ex: ex[\"id\"] in val_ids)\n",
    "\n",
    "    tr_set = train_feat.select(tr_idx.tolist())\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"/kaggle/working/fold{fold}\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=5,\n",
    "        gradient_checkpointing=False,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        fp16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        logging_first_step=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        disable_tqdm=False,\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=f\"/kaggle/working/fold{fold}/tb_logs\",\n",
    "        logging_nan_inf_filter=True,\n",
    "        seed=fold,\n",
    "        report_to=None\n",
    "    )\n",
    "\n",
    "    loss_tracker = LossTrackerCallback(\n",
    "        csv_path=f\"/kaggle/working/fold{fold}/loss_log.csv\"\n",
    "    )\n",
    "\n",
    "    eval_features = vl_set\n",
    "\n",
    "    metrics_fn = build_fast_metrics_fn(\n",
    "                 examples = raw_val,\n",
    "                 features = vl_set\n",
    "             )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tr_set,\n",
    "        eval_dataset=vl_set,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics = metrics_fn,\n",
    "        callbacks=[loss_tracker],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    fold_metrics = trainer.evaluate()\n",
    "    results.append(fold_metrics)\n",
    "    print(f\"Fold {fold+1} metrics:\", fold_metrics)\n",
    "\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jjefg6Mk-f5X"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "Training stopped because of kaggle disk limitation (Saving the checkpoints) but we reached to F1=56 and EM=40 after 5 epochs on the first folding. Kaggle cleaned the disk so we couldn't test the fine-tuned model on pqa_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fwrngac_VjY"
   },
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-20T04:38:58.782485Z",
     "iopub.status.idle": "2025-07-20T04:38:58.782796Z",
     "shell.execute_reply": "2025-07-20T04:38:58.782654Z",
     "shell.execute_reply.started": "2025-07-20T04:38:58.782640Z"
    },
    "id": "3c7UroV57iV9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"5-fold CV Results:\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7901095,
     "sourceId": 12517271,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7902173,
     "sourceId": 12518826,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
